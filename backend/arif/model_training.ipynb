{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"../mihiresh/commerce_logic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['years_to_retire', 'salary', 'investment_amount', 'current_savings', 'debt',\n",
    "            'other_expenses', 'number_of_dependents', 'current_invested_amount', 'house_ownership']\n",
    "\n",
    "# Targets for percentage allocations\n",
    "targets_low = ['s1_low', 's2_low', 's3_low', 's4_low', 's5_low', 's6_low']\n",
    "targets_mid = ['s1_mid', 's2_mid', 's3_mid', 's4_mid', 's5_mid', 's6_mid']\n",
    "targets_high = ['s1_high', 's2_high', 's3_high', 's4_high', 's5_high', 's6_high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = data1[features]\n",
    "y_low = data1[targets_low]\n",
    "y_mid = data1[targets_mid]\n",
    "y_high = data1[targets_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target values to sum to 1\n",
    "y_low = y_low.div(y_low.sum(axis=1), axis=0)\n",
    "y_mid = y_mid.div(y_mid.sum(axis=1), axis=0)\n",
    "y_high = y_high.div(y_high.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables\n",
    "categorical_features = ['house_ownership']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Transform the data\n",
    "X_processed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train_low, y_test_low = train_test_split(X_processed, y_low, test_size=0.2, random_state=42)\n",
    "_, _, y_train_mid, y_test_mid = train_test_split(X_processed, y_mid, test_size=0.2, random_state=42)\n",
    "_, _, y_train_high, y_test_high = train_test_split(X_processed, y_high, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models for each risk level\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test, risk_level):\n",
    "    models = {}\n",
    "    for target in y_train.columns:\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "        model.fit(X_train, y_train[target])\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test[target], y_pred)\n",
    "        models[target] = model\n",
    "        print(f'{risk_level} - MAE for {target}: {mae}')\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low - MAE for s1_low: 0.04145885862420769\n",
      "Low - MAE for s2_low: 0.0120480435030057\n",
      "Low - MAE for s3_low: 0.024526949663663027\n",
      "Low - MAE for s4_low: 0.011897970166384126\n",
      "Low - MAE for s5_low: 0.02435982686067356\n",
      "Low - MAE for s6_low: 0.02460444922108109\n",
      "Mid - MAE for s1_mid: 0.040972159837673605\n",
      "Mid - MAE for s2_mid: 0.011813653698194319\n",
      "Mid - MAE for s3_mid: 0.024224464764362047\n",
      "Mid - MAE for s4_mid: 0.023618627189040408\n",
      "Mid - MAE for s5_mid: 0.023873971176058256\n",
      "Mid - MAE for s6_mid: 0.023809097579230477\n",
      "High - MAE for s1_high: 0.051150638635850496\n",
      "High - MAE for s2_high: 0.024930724066321325\n",
      "High - MAE for s3_high: 0.01211885965743685\n",
      "High - MAE for s4_high: 0.012151120027945774\n",
      "High - MAE for s5_high: 0.03751380572969898\n",
      "High - MAE for s6_high: 0.025008064183815633\n"
     ]
    }
   ],
   "source": [
    "low_models = train_and_evaluate(X_train, X_test, y_train_low, y_test_low, 'Low')\n",
    "mid_models = train_and_evaluate(X_train, X_test, y_train_mid, y_test_mid, 'Mid')\n",
    "high_models = train_and_evaluate(X_train, X_test, y_train_high, y_test_high, 'High')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the models\n",
    "def predict(models, X):\n",
    "    predictions = {}\n",
    "    for target, model in models.items():\n",
    "        predictions[target] = model.predict(X)\n",
    "    return predictions\n",
    "\n",
    "low_predictions = predict(low_models, X_test)\n",
    "mid_predictions = predict(mid_models, X_test)\n",
    "high_predictions = predict(high_models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(employee_data, low_pred, mid_pred, high_pred):\n",
    "    print(\"Low Risk:\")\n",
    "    for i, investment in enumerate(low_pred.keys()):\n",
    "        print(f\"{investment}: {low_pred[investment][0]*100:.2f}%\")\n",
    "    print(\"Medium Risk:\")\n",
    "    for i, investment in enumerate(mid_pred.keys()):\n",
    "        print(f\"{investment}: {mid_pred[investment][0]*100:.2f}%\")\n",
    "    print(\"High Risk:\")\n",
    "    for i, investment in enumerate(high_pred.keys()):\n",
    "        print(f\"{investment}: {high_pred[investment][0]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Risk:\n",
      "s1_low: 7.78%\n",
      "s2_low: 2.08%\n",
      "s3_low: 24.37%\n",
      "s4_low: 16.84%\n",
      "s5_low: 39.51%\n",
      "s6_low: 9.41%\n",
      "Medium Risk:\n",
      "s1_mid: 5.73%\n",
      "s2_mid: 6.98%\n",
      "s3_mid: 14.38%\n",
      "s4_mid: 24.24%\n",
      "s5_mid: 19.24%\n",
      "s6_mid: 29.18%\n",
      "High Risk:\n",
      "s1_high: 31.16%\n",
      "s2_high: 29.58%\n",
      "s3_high: 7.01%\n",
      "s4_high: 6.99%\n",
      "s5_high: 6.29%\n",
      "s6_high: 19.03%\n"
     ]
    }
   ],
   "source": [
    "# Example new employee data\n",
    "new_employee = pd.DataFrame([[30, 45000, 12000, 35000, 2000, 3000, 1, 1, 'Own House']], columns=features)\n",
    "new_employee_processed = preprocessor.transform(new_employee)\n",
    "\n",
    "predicted_low = predict(low_models, new_employee_processed)\n",
    "predicted_mid = predict(mid_models, new_employee_processed)\n",
    "predicted_high = predict(high_models, new_employee_processed)\n",
    "\n",
    "display_results(new_employee, predicted_low, predicted_mid, predicted_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABixElEQVR4nO3deVxUVf8H8M/MsIuAym4I7uaKoiKWWknikqZZktkjmmWLmkZq0lNq5ROZlpZaZpb21FO4Lz+3UnJLURNc08gFl5TNBRCQbeb8/rjNyMCwDDL3AvN5v173NXPvnHvv98ydYb6cc+69KiGEABEREZEVUSsdABEREZHcmAARERGR1WECRERERFaHCRARERFZHSZAREREZHWYABEREZHVYQJEREREVocJEBEREVkdJkBERERkdZgAERECAgIwZswYpcOoc+bNm4dmzZpBo9EgMDBQ6XCqFT8zVNsxASKqJitXroRKpcLRo0eVDqXWycvLw4IFCxAcHAxXV1c4ODigVatWmDhxIv766y+lw6uSX375BdOnT8dDDz2EFStW4MMPP1Q6JPzyyy8YN24c2rdvD41Gg4CAAKVDIlKMjdIBEJHyEhMToVYr8//QjRs30L9/f8THx+OJJ57Ac889B2dnZyQmJiImJgbLli1DQUGBIrHdj19//RVqtRrffPMN7OzslA4HAPDjjz9i1apV6NKlC3x9fZUOh0hRTICI6piioiLodDqzfnTt7e0tGFH5xowZg2PHjmHt2rUYPny40WsffPAB/v3vf1fLfqryvtyPtLQ0ODo6Vtv+hBDIy8uDo6Njlbfx4Ycf4uuvv4atrS2eeOIJnD59ulpiI6qN2AVGJLNr167hhRdegJeXF+zt7dGuXTt8++23RmUKCgowc+ZMBAUFwdXVFfXq1UOvXr2we/duo3KXLl2CSqXC/PnzsXDhQjRv3hz29vY4c+YMZs+eDZVKhfPnz2PMmDFwc3ODq6srxo4di9zcXKPtlBzPoe/OO3DgACIjI+Hh4YF69eph2LBhSE9PN1pXp9Nh9uzZ8PX1hZOTEx599FGcOXOmUmNEDh8+jK1bt2LcuHGlkh9ASszmz59vmH/kkUfwyCOPlCo3ZswYo+6cst6XY8eOwcbGBu+9916pbSQmJkKlUmHx4sWGZRkZGZgyZQr8/Pxgb2+PFi1aYO7cudDpdOXWS6VSYcWKFcjJyYFKpYJKpcLKlSsBSInYBx98YIgpICAAb7/9NvLz8422ERAQgCeeeAI///wzunbtCkdHR3z11Vdl7vPcuXMYPnw4vL294eDggAceeADPPvssMjMzDWV8fX1ha2tbbuz34+LFi3jmmWfQsGFDODk5oUePHti6davhdSEE3N3dERkZaVim0+ng5uYGjUaDjIwMw/K5c+fCxsYG2dnZFouXrBtbgIhklJqaih49ekClUmHixInw8PDA9u3bMW7cOGRlZWHKlCkAgKysLCxfvhwjR47ESy+9hDt37uCbb75BWFgYjhw5UmpA7YoVK5CXl4fx48fD3t4eDRs2NLw2YsQING3aFNHR0UhISMDy5cvh6emJuXPnVhjvpEmT0KBBA8yaNQuXLl3CwoULMXHiRKxatcpQJioqCh9//DEGDx6MsLAwnDhxAmFhYcjLy6tw+5s3bwYA/Otf/6rEu2e+ku+Lj48P+vTpg9WrV2PWrFlGZVetWgWNRoNnnnkGAJCbm4s+ffrg2rVrePnll9GkSRMcPHgQUVFRSE5OxsKFC8vc7/fff49ly5bhyJEjWL58OQCgZ8+eAIAXX3wR3333HZ5++mm8+eabOHz4MKKjo3H27Fls2LDBaDuJiYkYOXIkXn75Zbz00kto3bq1yf0VFBQgLCwM+fn5mDRpEry9vXHt2jVs2bIFGRkZcHV1repbWGmpqano2bMncnNz8frrr6NRo0b47rvvMGTIEKxduxbDhg2DSqXCQw89hH379hnWO3nyJDIzM6FWq3HgwAEMGjQIALB//3507twZzs7OFo+drJQgomqxYsUKAUD8/vvvZZYZN26c8PHxETdu3DBa/uyzzwpXV1eRm5srhBCiqKhI5OfnG5W5ffu28PLyEi+88IJhWVJSkgAgXFxcRFpamlH5WbNmCQBG5YUQYtiwYaJRo0ZGy/z9/UVERESpuoSGhgqdTmdY/sYbbwiNRiMyMjKEEEKkpKQIGxsbMXToUKPtzZ49WwAw2qYpw4YNEwDE7du3yy2n16dPH9GnT59SyyMiIoS/v79hvrz35auvvhIAxKlTp4yWt23bVjz22GOG+Q8++EDUq1dP/PXXX0blZsyYITQajbhy5Uq5sUZERIh69eoZLTt+/LgAIF588UWj5VOnThUAxK+//mpY5u/vLwCIHTt2lLsfIYQ4duyYACDWrFlTYVm9QYMGGb1n5ir5mZkyZYoAIPbv329YdufOHdG0aVMREBAgtFqtEEKIefPmCY1GI7KysoQQQnz++efC399fdO/eXbz11ltCCCG0Wq1wc3MTb7zxRpXjI6oIu8CIZCKEwLp16zB48GAIIXDjxg3DFBYWhszMTCQkJAAANBqNYeyITqfDrVu3UFRUhK5duxrKFDd8+HB4eHiY3O8rr7xiNN+rVy/cvHkTWVlZFcY8fvx4qFQqo3W1Wi0uX74MAIiNjUVRURFee+01o/UmTZpU4bYBGGKoX79+pcqby9T78tRTT8HGxsaoFev06dM4c+YMwsPDDcvWrFmDXr16oUGDBkbHKjQ0FFqt1qgVo7K2bdsGAEZdQADw5ptvAoBRdxEANG3aFGFhYRVuV9/C8/PPP5fq3pTLtm3b0L17dzz88MOGZc7Ozhg/fjwuXbqEM2fOALj3GTp48CAAqaWnV69e6NWrF/bv3w9AOh4ZGRno1auX/BUhq8EEiEgm6enpyMjIwLJly+Dh4WE0jR07FoA0cFbvu+++Q8eOHeHg4IBGjRrBw8MDW7duNRrTode0adMy99ukSROj+QYNGgAAbt++XWHMFa2rT4RatGhhVK5hw4aGsuVxcXEBANy5c6fCslVh6n1xd3dH3759sXr1asOyVatWwcbGBk899ZRh2blz57Bjx45Sxyo0NBSA8bGqrMuXL0OtVpd6v7y9veHm5mZ4P8uL35SmTZsiMjISy5cvh7u7O8LCwrBkyRKTnxVLuXz5sskuugcffNDwOgB06dIFTk5OhmRHnwD17t0bR48eRV5enuG14skUUXXjGCAimegHzj7//POIiIgwWaZjx44AgB9++AFjxozB0KFDMW3aNHh6ekKj0SA6OhoXLlwotV55ZwZpNBqTy4UQFcZ8P+tWRps2bQAAp06dqtR/+yqVyuS+tVqtyfJlvS/PPvssxo4di+PHjyMwMBCrV69G37594e7ubiij0+nw+OOPY/r06Sa30apVqwrjLUvxVrXymHPG1yeffIIxY8Zg06ZN+OWXX/D6668jOjoahw4dwgMPPFDVUKudra0tgoODsW/fPpw/fx4pKSno1asXvLy8UFhYiMOHD2P//v1o06ZNma2aRNWBCRCRTDw8PFC/fn1otVpDK0JZ1q5di2bNmmH9+vVGP5YlB+4qzd/fHwBw/vx5o9aKmzdvVqqFafDgwYiOjsYPP/xQqQSoQYMGuHjxYqnlJVtOKjJ06FC8/PLLhm6wv/76C1FRUUZlmjdvjuzs7AqPlTn8/f2h0+lw7tw5Q8sIIA0gzsjIMLyfVdWhQwd06NAB77zzDg4ePIiHHnoIS5cuxZw5c+439Ar5+/sjMTGx1PI///zT8Lper169MHfuXOzatQvu7u5o06YNVCoV2rVrh/3792P//v144oknLB4zWTd2gRHJRKPRYPjw4Vi3bp3J668UP71c3/JSvLXj8OHDiIuLs3ygZujbty9sbGzw5ZdfGi0vfip5eUJCQtC/f38sX74cGzduLPV6QUEBpk6daphv3rw5/vzzT6P36sSJEzhw4IBZcbu5uSEsLAyrV69GTEwM7OzsMHToUKMyI0aMQFxcHH7++edS62dkZKCoqMisfQLAwIEDAaDUGWSffvopABjOgDJXVlZWqXg6dOgAtVpd6vR6Sxk4cCCOHDli9BnNycnBsmXLEBAQgLZt2xqW9+rVC/n5+Vi4cCEefvhhQ5Lfq1cvfP/997h+/TrH/5DFsQWIqJp9++232LFjR6nlkydPxkcffYTdu3cjODgYL730Etq2bYtbt24hISEBu3btwq1btwAATzzxBNavX49hw4Zh0KBBSEpKwtKlS9G2bdsadV0ULy8vTJ48GZ988gmGDBmC/v3748SJE9i+fTvc3d0r1dXz3//+F/369cNTTz2FwYMHo2/fvqhXrx7OnTuHmJgYJCcnG64F9MILL+DTTz9FWFgYxo0bh7S0NCxduhTt2rWr1KDu4sLDw/H888/jiy++QFhYGNzc3IxenzZtGjZv3ownnngCY8aMQVBQEHJycnDq1CmsXbsWly5dMuoyq4xOnTohIiICy5YtQ0ZGBvr06YMjR47gu+++w9ChQ/Hoo4+atT29X3/9FRMnTsQzzzyDVq1aoaioCN9//70h6dY7efKk4dID58+fR2ZmpqF1qFOnThg8eHCV9g8AM2bMwE8//YQBAwbg9ddfR8OGDfHdd98hKSkJ69atM7rSeEhICGxsbJCYmIjx48cblvfu3duQTDMBIotT8hQ0orpEf+p4WdPVq1eFEEKkpqaKCRMmCD8/P2Frayu8vb1F3759xbJlywzb0ul04sMPPxT+/v7C3t5edO7cWWzZsqXM073nzZtXKh79afDp6ekm40xKSjIsK+s0+JKn9O/evVsAELt37zYsKyoqEu+++67w9vYWjo6O4rHHHhNnz54VjRo1Eq+88kql3rvc3Fwxf/580a1bN+Hs7Czs7OxEy5YtxaRJk8T58+eNyv7www+iWbNmws7OTgQGBoqff/7ZrPdFLysrSzg6OgoA4ocffjBZ5s6dOyIqKkq0aNFC2NnZCXd3d9GzZ08xf/58UVBQUG6dTJ0GL4QQhYWF4r333hNNmzYVtra2ws/PT0RFRYm8vDyjcv7+/mLQoEHl7kPv4sWL4oUXXhDNmzcXDg4OomHDhuLRRx8Vu3btMipX3me0oksWlFTyMyOEEBcuXBBPP/20cHNzEw4ODqJ79+5iy5YtJtfv1q2bACAOHz5sWPb3338LAMLPz8+sWIiqQiVENY1mJCL6R0ZGBho0aIA5c+ZU260siIiqE8cAEdF9uXv3bqll+jEupm5bQURUE3AMEBHdl1WrVmHlypUYOHAgnJ2d8dtvv+Gnn35Cv3798NBDDykdHpkpJSWl3NcdHR1lubUGkaUxASKi+9KxY0fY2Njg448/RlZWlmFgtBynXlP18/HxKff1iIgIw41diWozjgEiIiKDXbt2lfu6r6+v0SntRLUVEyAiIiKyOhwETURERFaHY4BM0Ol0uH79OurXr1/pe/YQERGRsoQQuHPnDnx9fY0uvmkKEyATrl+/Dj8/P6XDICIioiq4evVqhTcBZgJkQv369QFIb6CLi4vC0RAREVFlZGVlwc/Pz/A7Xh4mQCbou71cXFyYABEREdUylRm+wkHQREREZHWYABEREZHVYQJEREREVodjgIiIiGSk0+lQUFCgdBi1kq2tLTQaTbVsiwkQERGRTAoKCpCUlASdTqd0KLWWm5sbvL297/s6fUyAiIiIZCCEQHJyMjQaDfz8/Cq8UB8ZE0IgNzcXaWlpACq+cW9FmAARERHJoKioCLm5ufD19YWTk5PS4dRKjo6OAIC0tDR4enreV3cY008iIiIZaLVaAICdnZ3CkdRu+uSxsLDwvrbDBIiIiEhGvMfk/amu948JEBEREVkdJkBEREQki4CAACxcuFDpMABwELRs7twBbt8GsrOBnBygYUOgeXOloyIiIirfI488gsDAwGpJXH7//XfUq1fv/oOqBmwBkklUFODvD7RrB3TvDsybp3RERERUayUnA7NnS48KE0KgqKioUmU9PDxqzBlwTIBkUjLhzc5WJg4iIqoDkpOB996zeAI0ZswY7N27F5999hlUKhVUKhVWrlwJlUqF7du3IygoCPb29vjtt99w4cIFPPnkk/Dy8oKzszO6deuGXbt2GW2vZBeYSqXC8uXLMWzYMDg5OaFly5bYvHmzReukxwRIJiUToJwcZeIgIiKqrM8++wwhISF46aWXkJycjOTkZPj5+QEAZsyYgY8++ghnz55Fx44dkZ2djYEDByI2NhbHjh1D//79MXjwYFy5cqXcfbz33nsYMWIETp48iYEDB2LUqFG4deuWxevGMUAycXY2nmcCRERE6NoVSEmpXFmtVpoAQH8NnL59AVtb6blGI02V4e0NHD1aYTFXV1fY2dnByckJ3t7eAIA///wTAPD+++/j8ccfN5Rt2LAhOnXqZJj/4IMPsGHDBmzevBkTJ04scx9jxozByJEjAQAffvghPv/8cxw5cgT9+/evXF2qiAmQTNgFRkREpaSkANeuVX39jIxqC8VcXbt2NZrPzs7G7NmzsXXrViQnJ6OoqAh3796tsAWoY8eOhuf16tWDi4uL4XYXlsQESCbsAiMiolL+aVWplJItQBkZgJtb1VuA7lPJs7mmTp2KnTt3Yv78+WjRogUcHR3x9NNPo6CgoNzt2Orj/4dKpZLlZrFMgGTCLjAiIiqlEt1QJiUkAEFBQGws0KVL9cZUgp2dneE2HuU5cOAAxowZg2HDhgGQWoQuXbpk0djuBwdBy4RdYEREVBsFBATg8OHDuHTpEm7cuFFm60zLli2xfv16HD9+HCdOnMBzzz0nS0tOVTEBkgm7wIiIqNr4+ACzZkmPFjZ16lRoNBq0bdsWHh4eZY7p+fTTT9GgQQP07NkTgwcPRlhYGLpYuHXqfqiEEELpIGqarKwsuLq6IjMzEy4uLtWyzZMngWKD46FSSV25vCceEZF1yMvLQ1JSEpo2bQoHBwelw6m1ynsfzfn9ZguQTEq2AAkB3L2rTCxERETWjgmQTEzd+oTdYERERMpgAiSTkmeBAUyAiIiIlMIESCam7v3GM8GIiIiUwQRIJmo14OhovIwtQERERMpgAiQjXgyRiIioZmACJCNeDJGIiKhmYAIkI14MkYiIqGZgAiQjdoERERHVDEyAZMQuMCIiopqBCZCM2AVGRES1zSOPPIIpU6ZU2/bGjBmDoUOHVtv2qooJkIzYBUZERFQz2CgdgDVhFxgREQGATgfcvKlsDI0aSdeoK8+YMWOwd+9e7N27F5999hkAICkpCdnZ2Zg2bRr279+PevXqoV+/fliwYAHc3d0BAGvXrsV7772H8+fPw8nJCZ07d8amTZswb948fPfddwAA1T93A9+9ezceeeQRi9WzLEyAZMQuMCIiAqTkx9NT2RjS0gAPj/LLfPbZZ/jrr7/Qvn17vP/++wAAW1tbdO/eHS+++CIWLFiAu3fv4q233sKIESPw66+/Ijk5GSNHjsTHH3+MYcOG4c6dO9i/fz+EEJg6dSrOnj2LrKwsrFixAgDQsGFDS1fVJCZAMmIXGBER1Saurq6ws7ODk5MTvL29AQBz5sxB586d8eGHHxrKffvtt/Dz88Nff/2F7OxsFBUV4amnnoK/vz8AoEOHDoayjo6OyM/PN2xPKUyAZMQuMCIiqu1OnDiB3bt3w9nEXb4vXLiAfv36oW/fvujQoQPCwsLQr18/PP3002jQoIEC0ZaNg6BlxC4wIiKq7bKzszF48GAcP37caDp37hx69+4NjUaDnTt3Yvv27Wjbti0WLVqE1q1bIykpSenQjbAFSEbsAiMiIkAagJyWpnwMlWFnZwetVmuY79KlC9atW4eAgADY2JhOI1QqFR566CE89NBDmDlzJvz9/bFhwwZERkaW2p5SmADJiF1gREQESGdfVTQAuaYICAjA4cOHcenSJTg7O2PChAn4+uuvMXLkSEyfPh0NGzbE+fPnERMTg+XLl+Po0aOIjY1Fv3794OnpicOHDyM9PR0PPvigYXs///wzEhMT0ahRI7i6usLW1lb2erELTEbsAiMiotpm6tSp0Gg0aNu2LTw8PFBQUIADBw5Aq9WiX79+6NChA6ZMmQI3Nzeo1Wq4uLhg3759GDhwIFq1aoV33nkHn3zyCQYMGAAAeOmll9C6dWt07doVHh4eOHDggCL1UgkhhCJ7rsGysrLg6uqKzMxMuLi4VNt29+wBHn303nyjRsCNG9W2eSIiqsHy8vKQlJSEpk2bwsHBQelwaq3y3kdzfr/ZAiQjdoERERHVDEyAZFQyAcrPB2rAODAiIiKrwwRIRiYumcBxQERERApgAiSjki1AALvBiIiIlMAESEamEiC2ABERWReee3R/quv9YwIkI3t7QKMxXsYEiIjIOmj++QEoKChQOJLaLTc3FwDu+9pBil8IccmSJZg3bx5SUlLQqVMnLFq0CN27dzdZ9o8//sDMmTMRHx+Py5cvY8GCBZgyZcp9bVNOKpXUCpSVdW8Zu8CIiKyDjY0NnJyckJ6eDltbW6jVbIMwhxACubm5SEtLg5ubmyGhrCpFE6BVq1YhMjISS5cuRXBwMBYuXIiwsDAkJibC09OzVPnc3Fw0a9YMzzzzDN54441q2abcSiZAbAEiIrIOKpUKPj4+SEpKwuXLl5UOp9Zyc3OrljvJK3ohxODgYHTr1g2LFy8GAOh0Ovj5+WHSpEmYMWNGuesGBARgypQppVqA7mebepa6ECIAtGoFnDt3b37dOuCpp6p1F0REVIPpdDp2g1WRra1tuS0/5vx+K9YCVFBQgPj4eERFRRmWqdVqhIaGIi4uTtZt5ufnIz8/3zCfVbyJpprxYohERNZNrVbzStA1gGIdkDdu3IBWq4WXl5fRci8vL6SkpMi6zejoaLi6uhomPz+/Ku2/Mng/MCIiIuVxBBaAqKgoZGZmGqarV69abF8lL4bIBIiIiEh+inWBubu7Q6PRIDU11Wh5ampqlQc3VXWb9vb2sLe3r9I+zcUuMCIiIuUp1gJkZ2eHoKAgxMbGGpbpdDrExsYiJCSkxmyzurELjIiISHmKngYfGRmJiIgIdO3aFd27d8fChQuRk5ODsWPHAgBGjx6Nxo0bIzo6GoA0yPnMmTOG59euXcPx48fh7OyMFi1aVGqbSmMXGBERkfIUTYDCw8ORnp6OmTNnIiUlBYGBgdixY4dhEPOVK1eMLhR1/fp1dO7c2TA/f/58zJ8/H3369MGePXsqtU2lsQuMiIhIeYpeB6imsuR1gGbPBt577978U09J1wIiIiKi+2PO7zfPApMZu8CIiIiUxwRIZuwCIyIiUh4TIJnxLDAiIiLlMQGSGbvAiIiIlMcESGbsAiMiIlIeEyCZsQuMiIhIeUyAZGaqC4wXIiAiIpIXEyCZlWwB0mqB/HxlYiEiIrJWTIBkVjIBAtgNRkREJDcmQDIr2QUGMAEiIiKSGxMgmZlqAeKZYERERPJiAiQzjQawtzdexhYgIiIieTEBUgAvhkhERKQsJkAK4MUQiYiIlMUESAG8GCIREZGymAApgF1gREREymICpAB2gRERESmLCZAC2AVGRESkLCZACmAXGBERkbKYACmAXWBERETKYgKkAHaBERERKYsJkALYBUZERKQsJkAKYBcYERGRspgAKYBdYERERMpiAqQAdoEREREpiwmQAtgFRkREpCwmQApgFxgREZGymAApgF1gREREymICpAB2gRERESmLCZACSiZAd+8COp0ysRAREVkjJkAKKNkFBgC5ufLHQUREZK2YACmgZAsQwG4wIiIiOTEBUoCpBIgDoYmIiOTDBEgBjo6ASmW8jAkQERGRfJgAKUCl4plgRERESmICpBBeDJGIiEg5TIAUwoshEhERKYcJkELYBUZERKQcJkAKYRcYERGRcpgAKYRdYERERMphAqQQdoEREREphwmQQtgFRkREpBwmQAphFxgREZFymAAphF1gREREymECpBB2gRERESmHCZBC2AVGRESkHCZACmEXGBERkXKYACmEXWBERETKUTwBWrJkCQICAuDg4IDg4GAcOXKk3PJr1qxBmzZt4ODggA4dOmDbtm1Gr2dnZ2PixIl44IEH4OjoiLZt22Lp0qWWrEKVsAuMiIhIOYomQKtWrUJkZCRmzZqFhIQEdOrUCWFhYUhLSzNZ/uDBgxg5ciTGjRuHY8eOYejQoRg6dChOnz5tKBMZGYkdO3bghx9+wNmzZzFlyhRMnDgRmzdvlqtalcIuMCIiIuWohBBCqZ0HBwejW7duWLx4MQBAp9PBz88PkyZNwowZM0qVDw8PR05ODrZs2WJY1qNHDwQGBhpaedq3b4/w8HC8++67hjJBQUEYMGAA5syZU6m4srKy4OrqiszMTLi4uNxPFcu0bx/Qp8+9eTc34PZti+yKiIjIKpjz+61YC1BBQQHi4+MRGhp6Lxi1GqGhoYiLizO5TlxcnFF5AAgLCzMq37NnT2zevBnXrl2DEAK7d+/GX3/9hX79+lmmIlXELjAiIiLlmJUAabVa7Nu3DxkZGfe94xs3bkCr1cLLy8touZeXF1JSUkyuk5KSUmH5RYsWoW3btnjggQdgZ2eH/v37Y8mSJejdu3eZseTn5yMrK8tosrSSXWCFhUBBgcV3S0RERDAzAdJoNOjXrx9u1+C+mkWLFuHQoUPYvHkz4uPj8cknn2DChAnYtWtXmetER0fD1dXVMPn5+Vk8zpIJEMBWICIiIrmY3QXWvn17XLx48b537O7uDo1Gg9TUVKPlqamp8Pb2NrmOt7d3ueXv3r2Lt99+G59++ikGDx6Mjh07YuLEiQgPD8f8+fPLjCUqKgqZmZmG6erVq/dZu4qV7AIDmAARERHJxewEaM6cOZg6dSq2bNmC5OTkKncd2dnZISgoCLGxsYZlOp0OsbGxCAkJMblOSEiIUXkA2Llzp6F8YWEhCgsLoVYbV0uj0UCn05UZi729PVxcXIwmSzPVAsQzwYiIiORhY+4KAwcOBAAMGTIEKpXKsFwIAZVKBa1WW+ltRUZGIiIiAl27dkX37t2xcOFC5OTkYOzYsQCA0aNHo3HjxoiOjgYATJ48GX369MEnn3yCQYMGISYmBkePHsWyZcsAAC4uLujTpw+mTZsGR0dH+Pv7Y+/evfjvf/+LTz/91NyqWpStrTQVFt5bxhYgIiIieZidAO3evbvadh4eHo709HTMnDkTKSkpCAwMxI4dOwwDna9cuWLUmtOzZ0/8+OOPeOedd/D222+jZcuW2LhxI9q3b28oExMTg6ioKIwaNQq3bt2Cv78//vOf/+CVV16ptriri7Oz8anvTICIiIjkoeh1gGoqOa4DBAB+fsDff9+b37oV+KeBjYiIiMxkzu+32S1AAJCRkYFvvvkGZ8+eBQC0a9cOL7zwAlxdXauyOavF+4EREREpw+xB0EePHkXz5s2xYMEC3Lp1C7du3cKnn36K5s2bIyEhwRIx1lm8GCIREZEyzG4BeuONNzBkyBB8/fXXsLGRVi8qKsKLL76IKVOmYN++fdUeZF3F+4EREREpw+wE6OjRo0bJDwDY2Nhg+vTp6Nq1a7UGV9exC4yIiEgZZneBubi44MqVK6WWX716FfXr16+WoKwFu8CIiIiUYXYCFB4ejnHjxmHVqlW4evUqrl69ipiYGLz44osYOXKkJWKss9gFRkREpAyzu8Dmz58PlUqF0aNHo6ioCABga2uLV199FR999FG1B1iXsQuMiIhIGWYlQFqtFocOHcLs2bMRHR2NCxcuAACaN28OJycniwRYl7ELjIiISBlmJUD6u8GfPXsWTZs2RYcOHSwVl1VgFxgREZEyFLsbPLELjIiISCmK3Q2e2AVGRESkFEXvBm/t2AVGRESkDEXvBm/t2AVGRESkDLMSoMLCQrz//vtYunQpWrZsaamYrAa7wIiIiJRh1hggW1tbnDx50lKxWB12gRERESnD7EHQzz//PL755htLxGJ1SiZAubmAEMrEQkREZE3MHgNUVFSEb7/9Frt27UJQUBDqlfgV//TTT6stuLquZBeYEMDduwCvKUlERGRZZidAp0+fRpcuXQAAf/31l9Frxc8Ko4qVbAECpG4wJkBERESWxbPAFGQqAeJAaCIiIsszewxQedLS0qpzc3WeqZYeJkBERESWV+kEyMnJCenp6Yb5QYMGITk52TCfmpoKHx+f6o2ujlOrSydBPBOMiIjI8iqdAOXl5UEUO0Vp3759uHv3rlEZwVOYzMaLIRIREcmvWrvAOAjafLwYIhERkfyqNQEi8/FiiERERPKrdAKkUqmMWnhKzlPVsAuMiIhIfpU+DV4IgVatWhmSnuzsbHTu3BlqtdrwOpmPXWBERETyq3QCtGLFCkvGYbXYBUZERCS/SidAERERlozDarELjIiISH4cBK0wdoERERHJjwmQwtgFRkREJD8mQApjFxgREZH8mAApjF1gRERE8qtyAlRQUIDExEQUFRVVZzxWh11gRERE8jM7AcrNzcW4cePg5OSEdu3a4cqVKwCASZMm4aOPPqr2AOs6tgARERHJz+wEKCoqCidOnMCePXvg4OBgWB4aGopVq1ZVa3DWgGOAiIiI5Ffp6wDpbdy4EatWrUKPHj2MboXRrl07XLhwoVqDswbsAiMiIpKf2S1A6enp8PT0LLU8JyeH9warAnaBERERyc/sBKhr167YunWrYV6f9CxfvhwhISHVF5mVYBcYERGR/MzuAvvwww8xYMAAnDlzBkVFRfjss89w5swZHDx4EHv37rVEjHVayQQoPx8oKgJszD4yREREVFlmtwA9/PDDOH78OIqKitChQwf88ssv8PT0RFxcHIKCgiwRY51WsgsMYCsQERGRpVWpnaF58+b4+uuvqzsWq1SyBQiQEiBXV/ljISIishZmtwBpNBqkpaWVWn7z5k1oNJpqCcqamEqAeCYYERGRZZmdAAkhTC7Pz8+HnZ3dfQdkbezsSo/3YRcYERGRZVW6C+zzzz8HIJ31tXz5cjgXG7yi1Wqxb98+tGnTpvojrONUKqkVKDPz3jImQERERJZV6QRowYIFAKQWoKVLlxp1d9nZ2SEgIABLly6t/gitQMkEiF1gREREllXpBCgpKQkA8Oijj2L9+vVo0KCBxYKyNrwYIhERkbzMPgts9+7dlojDqvFiiERERPIyOwF64YUXyn3922+/rXIw1or3AyMiIpKX2QnQ7du3jeYLCwtx+vRpZGRk4LHHHqu2wKwJu8CIiIjkZfZp8Bs2bDCatmzZgosXLyI8PBw9evQwO4AlS5YgICAADg4OCA4OxpEjR8otv2bNGrRp0wYODg7o0KEDtm3bVqrM2bNnMWTIELi6uqJevXro1q0brly5YnZscmEXGBERkbzMToBMbkStRmRkpOFMscpatWoVIiMjMWvWLCQkJKBTp04ICwszeaFFADh48CBGjhyJcePG4dixYxg6dCiGDh2K06dPG8pcuHABDz/8MNq0aYM9e/bg5MmTePfdd+Hg4HBfdbQkdoERERHJSyXKurKhmbZt24aIiAikp6dXep3g4GB069YNixcvBgDodDr4+flh0qRJmDFjRqny4eHhyMnJwZYtWwzLevTogcDAQMMp+M8++yxsbW3x/fffV7kuWVlZcHV1RWZmJlxcXKq8ncqaMAH44ot786+8Anz5pcV3S0REVKeY8/tt9higyMhIo3khBJKTk7F161ZERERUejsFBQWIj49HVFSUYZlarUZoaCji4uJMrhMXF1dq/2FhYdi4cSMAKYHaunUrpk+fjrCwMBw7dgxNmzZFVFQUhg4dWmYs+fn5yM/PN8xnZWVVuh7VgV1gRERE8jK7C+zYsWNG08mTJwEAn3zyCRYuXFjp7dy4cQNarRZeXl5Gy728vJCSkmJynZSUlHLLp6WlITs7Gx999BH69++PX375BcOGDcNTTz2FvXv3lhlLdHQ0XF1dDZOfn1+l61Ed2AVGREQkrzp1HSCdTgcAePLJJ/HGG28AAAIDA3Hw4EEsXboUffr0MbleVFSUUctSVlaWrEkQzwIjIiKSl9kJUHVxd3eHRqNBamqq0fLU1FR4e3ubXMfb27vc8u7u7rCxsUHbtm2Nyjz44IP47bffyozF3t4e9vb2ValGtWAXGBERkbwqlQB17twZKpWqUhtMSEioVDk7OzsEBQUhNjbWMD5Hp9MhNjYWEydONLlOSEgIYmNjMWXKFMOynTt3IiQkxLDNbt26ITEx0Wi9v/76C/7+/pWKSwnsAiMiIpJXpRKg8gYQ34/IyEhERESga9eu6N69OxYuXIicnByMHTsWADB69Gg0btwY0dHRAIDJkyejT58++OSTTzBo0CDExMTg6NGjWLZsmWGb06ZNQ3h4OHr37o1HH30UO3bswP/93/9hz549FqlDdWAXGBERkbwqlQDNmjXLIjsPDw9Heno6Zs6ciZSUFAQGBmLHjh2Ggc5XrlyBWn1vnHbPnj3x448/4p133sHbb7+Nli1bYuPGjWjfvr2hzLBhw7B06VJER0fj9ddfR+vWrbFu3To8/PDDFqlDdWAXGBERkbyqfB2g+Ph4nD17FgDQrl07dO7cuVoDU5Lc1wE6cAAonp/Vrw/IfCY+ERFRrWfR6wClpaXh2WefxZ49e+Dm5gYAyMjIwKOPPoqYmBh4eHhUKWhrZqoLTAigksOuiIiIyExmXwdo0qRJuHPnDv744w/cunULt27dwunTp5GVlYXXX3/dEjHWeSW7wHQ6oNh1GYmIiKiamd0CtGPHDuzatQsPPvigYVnbtm2xZMkS9OvXr1qDsxYlEyBAOhOsBt++jIiIqFYzuwVIp9PB1ta21HJbW1vDhQjJPCW7wAAOhCYiIrIksxOgxx57DJMnT8b169cNy65du4Y33ngDffv2rdbgrIWTU+llTICIiIgsx+wEaPHixcjKykJAQACaN2+O5s2bo2nTpsjKysKiRYssEWOdp9GU7u7ixRCJiIgsx+wxQH5+fkhISMCuXbvw559/ApBuNREaGlrtwVkTZ2cgL+/ePFuAiIiILKdK9wJTqVR4/PHH8fjjjwOQToOn+1OvHnDjxr15JkBERESWY3YX2Ny5c7Fq1SrD/IgRI9CoUSM0btwYJ06cqNbgrAnvB0ZERCQfsxOgpUuXws/PD4B0I9KdO3di+/btGDBgAKZNm1btAVoL3g+MiIhIPmZ3gaWkpBgSoC1btmDEiBHo168fAgICEBwcXO0BWgveD4yIiEg+ZrcANWjQAFevXgUgXRRRP/hZCAGtVlu90VkRdoERERHJx+wWoKeeegrPPfccWrZsiZs3b2LAgAEAgGPHjqFFixbVHqC1YBcYERGRfMxOgBYsWICAgABcvXoVH3/8MZz/+eVOTk7Ga6+9Vu0BWgt2gREREcnH7ATI1tYWU6dOLbX8jTfeqJaArBW7wIiIiORTpesAJSYmYtGiRTh79iwA6UKIkyZNQuvWras1OGvCLjAiIiL5mD0Iet26dWjfvj3i4+PRqVMndOrUCQkJCWjfvj3WrVtniRitArvAiIiI5GN2C9D06dMRFRWF999/32j5rFmzMH36dAwfPrzagrMm7AIjIiKSj9ktQMnJyRg9enSp5c8//zySk5OrJShrxC4wIiIi+ZidAD3yyCPYv39/qeW//fYbevXqVS1BWSN2gREREcmnUl1gmzdvNjwfMmQI3nrrLcTHx6NHjx4AgEOHDmHNmjV47733LBOlFWAXGBERkXxUQghRUSG1unINRSqVqk5cDTorKwuurq7IzMyEi4uLLPvcvRt47LF78+7uQHq6LLsmIiKqE8z5/a5UC5BOp6uWwKhs7AIjIiKSj9ljgMqSkZGBxYsXV9fmrE7JBOjuXaAONKYRERHVSPedAMXGxuK5556Dj48PZs2aVR0xWaWSZ4EBQG6u/HEQERFZgyolQFevXsX777+Ppk2bol+/flCpVNiwYQNSUlKqOz6rUbIFCGA3GBERkaVUOgEqLCzEmjVrEBYWhtatW+P48eOYN28e1Go1/v3vf6N///6wtbW1ZKx1mqkEiGeCERERWUalrwTduHFjtGnTBs8//zxiYmLQoEEDAMDIkSMtFpw1cXAA1Gqg+HhztgARERFZRqVbgIqKiqBSqaBSqaDRaCwZk1VSqXgmGBERkVwqnQBdv34d48ePx08//QRvb28MHz4cGzZsgEqlsmR8VoUXQyQiIpJHpRMgBwcHjBo1Cr/++itOnTqFBx98EK+//jqKiorwn//8Bzt37qwTF0FUEu8HRkREJI8qnQXWvHlzzJkzB5cvX8bWrVuRn5+PJ554Al5eXtUdn1VhFxgREZE8Kj0I2hS1Wo0BAwZgwIABSE9Px/fff19dcVkldoERERHJo9quBO3h4YHIyMjq2pxVYhcYERGRPKotAaL7xy4wIiIieTABqkHYBUZERCQPJkA1CLvAiIiI5MEEqAZhFxgREZE8zD4LTKvVYuXKlYiNjUVaWhp0xe/dAODXX3+ttuCsDbvAiIiI5GF2AjR58mSsXLkSgwYNQvv27Xkl6GrELjAiIiJ5mJ0AxcTEYPXq1Rg4cKAl4rFqbAEiIiKSh9kJkJ2dHVq0aGGJWKxeyQTot9+A5s2BLl2AoCBp6tIFaNRImfiIiIjqCrMToDfffBOfffYZFi9ezO6vala/fullFy9K09q195b5+0vJUMeOQEDAvalxY8Dmvq7tTUREZB1UQghhzgrDhg3D7t270bBhQ7Rr1w62trZGr69fv75aA1RCVlYWXF1dkZmZCRcXF9n2e/OmlMTk51dtfY0GeOABKUEqnhQ5OwNOTlILU8lJv5yJExER1Xbm/H6b/bPn5uaGYcOGVTk4KlujRsC2bcC77wJHjwIFBeatr9UCly9L07595q1brx7g5ga4ukqPxZ+7ukqTjQ2gUgFqtfRYfNIvs7eXkqryJjs7qbx+Hf3z4tslIiKyJLNbgKyBUi1AxRUWAn/8AcTHAwkJ0uOJE0BeniLhyE6jkSYbG+PJ1DJbW2mqzHNTk/51/bZNPWo09xK08uh0UiJa0QQA+m9e8W9g8ef6pFC/7+JxmLOsskllyTKm5ksmq8XnSyaxZSXJKtW996moyPh9KT6vfy/K2p6pGCtTv+KPhue3bkK9bQvUQ56A2qORyXqpb6VDtX49VMOfgsrTw3RM6elQrVkN3dMjINw9IAQMk073z/P0G8D69VAPHwa1l4fpfd1IgyrmJ4hnR0J4eBptx2hKS4dYvQbi6Wega1T2/lQbN0j783Qve3+rYqAb8WzZ+0tLh1izFnj6acDDo9TnFZDqj3XrgOHDAQ8Po+Nj9P6nS9vSPfW0IW6d7t4kBKBLuwFs3gwMGQK4u5s8frhxA9i4EaphQwF3d9MHvlgZlYe7yc+Q6kY6VOvXQf3McKNjW/wzq1YDqvQ0iJ+k90nn7llG3OnAuvXA8KcAd48yYpLK6IYNh2jkfu9YFTtuuvSbhvqr3BuZjvvmDag2ma6/0Xfjxg1gwwZg2DBDuVLH7p8y6uHDoPJwN/qOGx5vSJ9vjBhR/mdgtXGZUoqVadjaAz4+potVhTm/30yATKgJCZApRUXA2bP3kqILF4BLl6QWH54yT0REtc3rrwOffVZ927NoFxgArF27FqtXr8aVK1dQUKKfJiEhoSqbpEqwsQE6dJCmMWPuLRdCGj90+bKUEOmToqQkKanPySk9Me0lIiJrZnYC9Pnnn+Pf//43xowZg02bNmHs2LG4cOECfv/9d0yYMKFKQSxZsgTz5s1DSkoKOnXqhEWLFqF79+5lll+zZg3effddXLp0CS1btsTcuXPLvC7RK6+8gq+++goLFizAlClTqhRfTadSSa2a7u7S2WEVEULqStMnQ9nZQGamNGVkSJP+uf4xK+tet4RR83qJJvf8fCA3t/RUVGTZ94CIiGqhtDQg4W/AxwfV2hdWCWYnQF988QWWLVuGkSNHYuXKlZg+fTqaNWuGmTNn4tatW2YHsGrVKkRGRmLp0qUIDg7GwoULERYWhsTERHh6epYqf/DgQYwcORLR0dF44okn8OOPP2Lo0KFISEhA+/btjcpu2LABhw4dgq+vr9lx1WUqFeDoKE1ldZuXkpwMfPUV8PLLZX9IyylTWAjcvQvkXExF4Xc/Qjw3Cjp3T+O+c/3ztHQUxayFdtjTKGrggaKie2ND9M+LEi+g8I1pKIqej0K/ZigslJYXFsL4+eXrKPzyaxS98DIKG3kbXjea0m5B+0ssih55HNr6bkb7MuwzMwc4fQpo36HYBZsEoBOATisFnp0D1blEaFq1gMbZARqVgEatK/aogyb3DjQnEqDq3Fm67oFaBZX6Xue+9FwNkZUFEXcI2qDu0DnUg7ZIB22RgK5IB20RoNPqoM3Jg/bqdei8G0NrYwedFtDqAK1WJY2x0amgLdRB5OQATvUAjRoQ/8QN3GsG1Oog8vKkEewqFSCE9JJRdiugE4CACjqoDVPJeS00EFCVOenXUUMHGxRBA61hKjmvhq7M7Ui1kJ6rYNycWXJeX7b4Oqa2U7IuJetXft1M31ZRBd0/r+qMSuv3e29fmrK/e5WghrZENKX3V7Ju5u5TVWybxstNNyfr39/iz+89qqH+5xjrJ33cxeeLv1emHks+NzWvX1bR5+jeZ8D8Y1G8LqbeI1P034Pi66hNfF7K+z4Vr0d570HJ70lZz0t+D4p/Z+/3M1qmmJ+AmCnArFnA7NmW2UcZzB4D5OTkhLNnz8Lf3x+enp7YuXMnOnXqhHPnzqFHjx64efOmWQEEBwejW7duWLx4MQBAp9PBz88PkyZNwowZM0qVDw8PR05ODrZs2WJY1qNHDwQGBmLp0qWGZdeuXUNwcDB+/vlnDBo0CFOmTKl0C1BNHQOkqIQEqXkpPl66GmNNKbNvH9CyZek+vtxc6fHsWeCDD4C33pKuEaAfYVt8tPLffwNffgmMGiUlJbm5UrZW/PHmTeDcOcDTU8qI7t6VJqp9vLyka0ToR5Ta2UmflaNHgW7dpFMfAek/BH1zKQDcuiV9Jrt3l07ZVKulZFitlpo+VSqIGzch9u2D+rFHpf2oVP8kumppH2q19B/vtm3AE09I16lQqwFXVwiVGro7OdCpbaC7ngLd2nVQDR8OlY83VEIHlZMjVPWcoMrKlCZtEVQpycD27cCAAdL+gHv31NFfSj41VSrz+ONS3FqtlOwWFUFk3YFOK6BLvwldwjGog7pA1aghVGqVtC+1Cqr8PGk0/c2b0vft0UcBb29pmYuL9JibKz2mpgKbNkmDoP39pWUNG0r7vXNHikmjAa5cAZYtAyZMkL6/Go00YNbGRmpy1mik79uHHwJvvw20bi0dC3d36fHGDekxMRGYO1cq066dtJ4+tlu3pMczZ4Dp04FPPwUCA6X3W/++p6dLj8ePA+PHSzEFBkr/BLh7QHh6QXf1GsT1ZAitDrrjJ6F+ayrU8z6GulMH6SPk7XXvfQaAkyeByEjg88+lv1/665Oo1UBKilQmIQF46SXg66/v/Y3T/9OYnCw9xsdLMS1dCnTufO+zK8S9MseOSe/hkiVAp07SMm9vaVvJyff2d/w4MHEi8MUX97oJGjeWPp9mxCTiEyDGj4dq2TJDGZVvibgrU7eyylRDC5BFxwB5e3vj1q1b8Pf3R5MmTXDo0CF06tQJSUlJMHc8dUFBAeLj4xEVFWVYplarERoairi4OJPrxMXFITIy0mhZWFgYNm7caJjX6XT417/+hWnTpqFdu3YVxpGfn4/8YhffycrKMqsetVpFLTtCSFdi1N/k9pdfpNHXpk6luXhRejxyRPpB0DczFZ/0p0Clp0t/mDIygNu3jafERKnMjBn3fpxycqSyd+7805T0z6jv3r0rV8+5cysu87//VVwmLa1y+6vtNBrpeOlPtdMPNHvgAaBBA8DBQfrxa9jwXvOdg4N0/LZvl34k69eXPgdqtXTMsrKkazvcvg1cuyZd5tzdXfox1m9TCGn+9m1gyxYgPFwqZ28v/WG3t5d+bO3tpUFuc+YA//mP9KNlayvFZ2cnxWpjU/k/tPqEeunSipPuL78ss4wqIQGqoCBg3rzyt7NtG/Dee0ZlVAA0/0xISADW/g94O7Ls7ejLbd8uvQ/l7W/7duCjj0qVMexTX7dlSyqu//z55ZfZtElKSCqKe9ky4IUXyt/Whx9KyVR5ZebOLb+M/lj36VO6TLNm0qPmn9YN/eX2Ib03KgDqB1sCD7aUXnd3AVAIPNa79Lb0PRANGkiPDz1UukzJ3oguXUqXKfnZ7NatdJmAAOnR3l567NGjdJnmzaUJkL7LABAcfF8xSe+JAIIqEXdl6maqjIzMToAee+wxbN68GZ07d8bYsWPxxhtvYO3atTh69Cieeuops7Z148YNaLVaeOn/c/mHl5cX/vzzT5PrpKSkmCyfos9iAcydOxc2NjZ4/fXXKxVHdHQ03nvvPbNirzOSk6U/xEOGSB/OjAwpgTl8GNizR/oDk5Fxr3yxZLVMr75acZn+/Ssus3NnxWWU5Osr/ZerUkk/3jk50vvWu7f0n5paLSUJ+gRAo5ESqK1bgYEDpdYknU76I+boKCUIWVnSsvR04MABoG9f6T81e3sp2XBwkFok7OyA69eBb76R/gNs1UpKALy9pR/+rCzp8a+/gJkzgehoafS8SiUdZ/1/vyoVcPo0MGWK9N9h165SnCWTBP2P36ZNlftBruhHMihIOg22vDJbtkj/uZdXZs4c6bNk6o9/cQr/oSWimsfsBGjZsmXQ6XQAgAkTJqBRo0Y4ePAghgwZgpdffrnaAzRXfHw8PvvsMyQkJFT6Vh1RUVFGrUpZWVnw8/OzVIjyqah1p6gI0Ceas2YB58/fm6+JHB2lBMDRUfrhvnYNaNNGSiQcHaVmdg8PqRWhqEhalpYGfP89MHas9N+eWi2V02juNbVfvAgsXix1k3XsKCUbTZpI62dmSsnGiRMVtyTof9gXLCj/R3vrVqlbrqJEIigI+Pjj8rf1zTcV/xc9cybQr1/ZZfT/sZr679Ba+PhI34HymuBrWpmaGBPjrnllamJMlY3bwhS9DlBBQQGcnJywdu1aDB061LA8IiICGRkZ2LRpU6l1mjRpgsjISKPxPLNmzcLGjRtx4sQJLFy4EJGRkVCr7w1K1Gq1UKvV8PPzw6VLlyqMq86MASo5lubvv4FDh4DYWKml4syZiu+74eIiNe02bCj9R/7MM1KLhBDSa0JIrQ2AtP3166XxCG5u97o/8vKkRCIvT+qXv3BB6q/285O24esrXWpaCKnbJCVFataOjpaafx0cgKZNjZtqKzNOqLLlalqZmhhTZQbBV7ZcTStDRHWGWb/fogr27dsnRo0aJXr06CH+/vtvIYQQ//3vf8X+/fvN3lb37t3FxIkTDfNarVY0btxYREdHmyw/YsQI8cQTTxgtCwkJES+//LIQQogbN26IU6dOGU2+vr7irbfeEn/++WelYsrMzBQARGZmptn1qVFWrZLO4Xn0USF8fcu8mGypqVs3Ib7/Xohz54TQ6aRtxcdLr8XHl72/mlZG7v1dvy7ErFnS4/2Uqc5tVWdMREQ1nDm/32YnQGvXrhWOjo7ixRdfFPb29uLChQtCCCEWLVokBgwYYHawMTExwt7eXqxcuVKcOXNGjB8/Xri5uYmUlBQhhBD/+te/xIwZMwzlDxw4IGxsbMT8+fPF2bNnxaxZs4Stra04depUmfvw9/cXCxYsqHRMtToBun5diNWrhXjssYoTHR8fIbp2lZ7PmCHEwYPSD7ypH8KaltzUxESCiIgUZdEEKDAwUHz33XdCCCGcnZ0NCVBCQoLw8vIyd3NCCCl5atKkibCzsxPdu3cXhw4dMrzWp08fERERYVR+9erVolWrVsLOzk60a9dObN26tdztW00ClJgoRPv25Sc9Dz8sxKZNQvyTYFa6JYUtEkREVMOZ8/tdpesAnTlzBgEBAahfvz5OnDiBZs2a4eLFi2jbti3y6sDdOmvFGKDiYxvy8oD33wf++1/pDCK9+vWl08Znz5bOOirv7J6KxqQQERHVcBa/DtD58+cRoL8OwT9+++03NNNfU4EsT3/6+qlT0h2Di99rolEj6Yymnj2Bhx8GBg8u/xoZNWA0PhERkZxMX7+9HC+99BImT56Mw4cPQ6VS4fr16/jf//6HqVOn4tXKXP+F7l9eHvDJJ9Lz9evvJT9ubtJ1UZKSgGnT7l38qjw+PlILERMgIiKyIma3AM2YMQM6nQ59+/ZFbm4uevfuDXt7e0ydOhWTJk2yRIykl5wsXbPmjTeA33+/t9zJCXjuOeDNN6Xr4uixdYeIiMikKl8HqKCgAOfPn0d2djbatm0LZ/39Z+qAGjsGaMaM8m/poMDN5IiIiGoKi44B0rOzs0Pbtm2rujqZ69Yt6RYDeo6O0u0VTN1wjoiIiMpV6QTohRdeqFS5b7/9tsrBUBnS0qQ7OZ88Kc03agR89hnw/PO8xxEREVEVVDoBWrlyJfz9/dG5c2ez7/pO9+HaNemGmPo7pHt5Abt2SXfVJiIioiqpdAL06quv4qeffkJSUhLGjh2L559/Hg0bNrRkbHTpkpT8XLwozT/wgHQfr1atpAHRHOBMRERUJWYNgs7Pz8f69evx7bff4uDBgxg0aBDGjRuHfv36VfrO67WB4oOgk5OBDz8E1q2TngPSncxjY4ES118iIiIiiTm/31U+C+zy5ctYuXIl/vvf/6KoqAh//PFHnTkTTPEEaPVqIDz83nybNlK3V+PG8sdCRERUS5jz+232hRANK6rVUKlUEEJAq9VWdTNU0rlzwPjx9+Y7dgT27mXyQ0REVI3MSoDy8/Px008/4fHHH0erVq1w6tQpLF68GFeuXKkzrT+KSU6W7sv1zjtAZqa0LCAAWLgQ+Pvve11hREREdN8qPQj6tddeQ0xMDPz8/PDCCy/gp59+gru7uyVjsy5ffSXd26u4S5eAxx6TnvMih0RERNWm0mOA1Go1mjRpgs6dO5c74Hn9+vXVFpxSFBkDlJwsTaNGAX/+KS376iuga1fpecm7uBMREZERi1wJevTo0XXqTK8aR5/g3Llzb1nXrrzIIRERkQWYdSFEsjCdDkhNVToKIiKiOq/KZ4GRBdy+DRQVSc+bN2eXFxERkYUwAapJUlLuPX/4YSZAREREFsIEqCYp3v3l7a1cHERERHUcE6CapHgLkJeXcnEQERHVcUyAahK2ABEREcmCCVBNwhYgIiIiWTABqknYAkRERCQLJkA1CVuAiIiIZMEEqCbRtwDZ2gINGigbCxERUR3GBKgm0bcAeXoCah4aIiIiS+GvbE2h1QLp6dJzdn8RERFZFBOgmuLmTSkJAjgAmoiIyMKYANUUxc8AYwsQERGRRTEBqil4CjwREZFsmADVFDwFnoiISDZMgGoKtgARERHJhglQTcEWICIiItkwAaop2AJEREQkGyZANQVbgIiIiGTDBKim0LcA2dkBbm6KhkJERFTXMQGqKfQtQF5egEqlbCxERER1HBOgmkCrBW7ckJ5z/A8REZHFMQGqCdLTAZ1Oes7xP0RERBbHBKgm4BlgREREsmICVBPwDDAiIiJZMQGqCdgCREREJCsmQDUBW4CIiIhkxQSoJmALEBERkayYANUEbAEiIiKSFROgmoAtQERERLJiAlQT6FuAHByA+vWVjYWIiMgKMAGqCfQtQLwNBhERkSxqRAK0ZMkSBAQEwMHBAcHBwThy5Ei55desWYM2bdrAwcEBHTp0wLZt2wyvFRYW4q233kKHDh1Qr149+Pr6YvTo0bh+/bqlq1E1hYXAzZvSc3Z/ERERyULxBGjVqlWIjIzErFmzkJCQgE6dOiEsLAxpaWkmyx88eBAjR47EuHHjcOzYMQwdOhRDhw7F6dOnAQC5ublISEjAu+++i4SEBKxfvx6JiYkYMmSInNWqvPR0QAjpOQdAExERyUIlhP7XVxnBwcHo1q0bFi9eDADQ6XTw8/PDpEmTMGPGjFLlw8PDkZOTgy1bthiW9ejRA4GBgVi6dKnJffz+++/o3r07Ll++jCZNmlQYU1ZWFlxdXZGZmQkXF5cq1qySjh0DunSRno8fD3z1lWX3R0REVEeZ8/utaAtQQUEB4uPjERoaalimVqsRGhqKuLg4k+vExcUZlQeAsLCwMssDQGZmJlQqFdzc3Kol7mrFU+CJiIhkZ6Pkzm/cuAGtVguvEj/8Xl5e+PPPP02uk5KSYrJ8SvFEopi8vDy89dZbGDlyZJnZYH5+PvLz8w3zWVlZ5lTj/vAUeCIiItkpPgbIkgoLCzFixAgIIfDll1+WWS46Ohqurq6Gyc/PT74g2QJEREQkO0UTIHd3d2g0GqQWbwUBkJqaCu8yWkO8vb0rVV6f/Fy+fBk7d+4sty8wKioKmZmZhunq1atVrFEVsAWIiIhIdoomQHZ2dggKCkJsbKxhmU6nQ2xsLEJCQkyuExISYlQeAHbu3GlUXp/8nDt3Drt27UKjRo3KjcPe3h4uLi5Gk2zYAkRERCQ7RccAAUBkZCQiIiLQtWtXdO/eHQsXLkROTg7Gjh0LABg9ejQaN26M6OhoAMDkyZPRp08ffPLJJxg0aBBiYmJw9OhRLFu2DICU/Dz99NNISEjAli1boNVqDeODGjZsCDs7O2UqWha2ABEREclO8QQoPDwc6enpmDlzJlJSUhAYGIgdO3YYBjpfuXIFavW9hqqePXvixx9/xDvvvIO3334bLVu2xMaNG9G+fXsAwLVr17B582YAQGBgoNG+du/ejUceeUSWelWavgXIyQlwdlY2FiIiIiuh+HWAaiJZrwPUqBFw6xbQrBlw4YJl90VERFSH1ZrrAFm9ggIp+QE4/oeIiEhGTICUVPx2Hxz/Q0REJBsmQEriGWBERESKYAKkJJ4BRkREpAgmQEpiCxAREZEimAApiS1AREREimACpCS2ABERESmCCZCSircAMQEiIiKSDRMgJTEBIiIiUgQTICXpu8CcnYF69ZSNhYiIyIowAVKSvgWIA6CJiIhkxQRIKXl5QEaG9JzdX0RERLJiAqQU3gaDiIhIMUyAlMJT4ImIiBTDBEgpvAgiERGRYpgAKYUtQERERIphAqQUtgAREREphgmQUtgCREREpBgmQEphCxAREZFimAAphS1AREREimECpBR9C5CrK+DgoGwsREREVoYJkFL0LUBs/SEiIpIdEyAl5OYCd+5Izzn+h4iISHZMgJRQfAA0W4CIiIhkxwRICTwDjIiISFFMgJTAM8CIiIgUxQRICWwBIiIiUhQTICVwDBAREZGimAApgV1gREREimICpAR2gRERESmKCZASircAeXoqFwcREZGVYgKkBH0LUIMGgL29srEQERFZISZASuBtMIiIiBTFBEhu2dlATo70nON/iIiIFMEESG48BZ6IiEhxTIDkxjPAiIiIFMcESG68BhAREZHimADJjS1AREREimMCJDe2ABERESmOCZDc2AJERESkOCZAcmMLEBERkeKYAMmteAsQb4NBRESkCCZActO3ADVqBNjaKhsLERGRlWICJCch7rUAcfwPERGRYpgAyenOHeDuXek5x/8QEREphgmQnHgGGBERUY3ABEhOvA8YERFRjcAESE48BZ6IiKhGYAIkJ3aBERER1Qg1IgFasmQJAgIC4ODggODgYBw5cqTc8mvWrEGbNm3g4OCADh06YNu2bUavCyEwc+ZM+Pj4wNHREaGhoTh37pwlq1A5xVuAeAo8ERGRYhRPgFatWoXIyEjMmjULCQkJ6NSpE8LCwpCWlmay/MGDBzFy5EiMGzcOx44dw9ChQzF06FCcPn3aUObjjz/G559/jqVLl+Lw4cOoV68ewsLCkJeXJ1e1TCveAqRSKRcHERGRlVMJIYSSAQQHB6Nbt25YvHgxAECn08HPzw+TJk3CjBkzSpUPDw9HTk4OtmzZYljWo0cPBAYGYunSpRBCwNfXF2+++SamTp0KAMjMzISXlxdWrlyJZ599tsKYsrKy4OrqiszMTLi4uFRTTQEMGQL83/9Jz3fsAMLCqm/bREREVs6c329FW4AKCgoQHx+P0NBQwzK1Wo3Q0FDExcWZXCcuLs6oPACEhYUZyiclJSElJcWojKurK4KDg8vcZn5+PrKysoymapWcDCQkABcv3lt26ZK0LCFBep2IiIhko2gCdOPGDWi1WniVOCPKy8sLKcXHyxSTkpJSbnn9oznbjI6Ohqurq2Hy8/OrUn3K9NVXQFAQ8Mcf95a98oq0LChIep2IiIhko/gYoJogKioKmZmZhunq1avVu4OXXwbi44HvvgNee01a9vXX0rL4eOl1IiIiko2Nkjt3d3eHRqNBavHBwQBSU1PhXcZp4t7e3uWW1z+mpqbCx8fHqExgYKDJbdrb28Pe3r6q1aiYj480dekCtG8PfPGF9LxLF8vtk4iIiMqkaAuQnZ0dgoKCEBsba1im0+kQGxuLkJAQk+uEhIQYlQeAnTt3Gso3bdoU3t7eRmWysrJw+PDhMrdJRERE1kXRFiAAiIyMREREBLp27Yru3btj4cKFyMnJwdixYwEAo0ePRuPGjREdHQ0AmDx5Mvr06YNPPvkEgwYNQkxMDI4ePYply5YBAFQqFaZMmYI5c+agZcuWaNq0Kd599134+vpi6NChSlXzHh8fYNYs6ZGIiIgUoXgCFB4ejvT0dMycORMpKSkIDAzEjh07DIOYr1y5ArX6XkNVz5498eOPP+Kdd97B22+/jZYtW2Ljxo1o3769ocz06dORk5OD8ePHIyMjAw8//DB27NgBBwcH2etXio8PMHu20lEQERFZNcWvA1QTWew6QERERGQxteY6QERERERKYAJEREREVocJEBEREVkdJkBERERkdZgAERERkdVhAkRERERWhwkQERERWR0mQERERGR1mAARERGR1VH8Vhg1kf7i2FlZWQpHQkRERJWl/92uzE0umACZcOfOHQCAn5+fwpEQERGRue7cuQNXV9dyy/BeYCbodDpcv34d9evXh0qlqtZtZ2Vlwc/PD1evXq2T9xlj/Wq/ul5H1q/2q+t1ZP2qTgiBO3fuwNfX1+hG6qawBcgEtVqNBx54wKL7cHFxqZMfbD3Wr/ar63Vk/Wq/ul5H1q9qKmr50eMgaCIiIrI6TICIiIjI6jABkpm9vT1mzZoFe3t7pUOxCNav9qvrdWT9ar+6XkfWTx4cBE1ERERWhy1AREREZHWYABEREZHVYQJEREREVocJEBEREVkdJkAyWrJkCQICAuDg4IDg4GAcOXJE6ZBKmT17NlQqldHUpk0bw+t5eXmYMGECGjVqBGdnZwwfPhypqalG27hy5QoGDRoEJycneHp6Ytq0aSgqKjIqs2fPHnTp0gX29vZo0aIFVq5cabE67du3D4MHD4avry9UKhU2btxo9LoQAjNnzoSPjw8cHR0RGhqKc+fOGZW5desWRo0aBRcXF7i5uWHcuHHIzs42KnPy5En06tULDg4O8PPzw8cff1wqljVr1qBNmzZwcHBAhw4dsG3bNovXb8yYMaWOaf/+/WtN/aKjo9GtWzfUr18fnp6eGDp0KBITE43KyPm5tMT3uDJ1fOSRR0odx1deeaVW1PHLL79Ex44dDRe+CwkJwfbt2w2v1/bjV1H9avOxM+Wjjz6CSqXClClTDMtq5TEUJIuYmBhhZ2cnvv32W/HHH3+Il156Sbi5uYnU1FSlQzMya9Ys0a5dO5GcnGyY0tPTDa+/8sorws/PT8TGxoqjR4+KHj16iJ49expeLyoqEu3btxehoaHi2LFjYtu2bcLd3V1ERUUZyly8eFE4OTmJyMhIcebMGbFo0SKh0WjEjh07LFKnbdu2iX//+99i/fr1AoDYsGGD0esfffSRcHV1FRs3bhQnTpwQQ4YMEU2bNhV37941lOnfv7/o1KmTOHTokNi/f79o0aKFGDlypOH1zMxM4eXlJUaNGiVOnz4tfvrpJ+Ho6Ci++uorQ5kDBw4IjUYjPv74Y3HmzBnxzjvvCFtbW3Hq1CmL1i8iIkL079/f6JjeunXLqExNrl9YWJhYsWKFOH36tDh+/LgYOHCgaNKkicjOzjaUketzaanvcWXq2KdPH/HSSy8ZHcfMzMxaUcfNmzeLrVu3ir/++kskJiaKt99+W9ja2orTp08LIWr/8auofrX52JV05MgRERAQIDp27CgmT55sWF4bjyETIJl0795dTJgwwTCv1WqFr6+viI6OVjCq0mbNmiU6depk8rWMjAxha2sr1qxZY1h29uxZAUDExcUJIaQfY7VaLVJSUgxlvvzyS+Hi4iLy8/OFEEJMnz5dtGvXzmjb4eHhIiwsrJprU1rJBEGn0wlvb28xb948w7KMjAxhb28vfvrpJyGEEGfOnBEAxO+//24os337dqFSqcS1a9eEEEJ88cUXokGDBoY6CiHEW2+9JVq3bm2YHzFihBg0aJBRPMHBweLll1+2WP2EkBKgJ598ssx1alP9hBAiLS1NABB79+4VQsj7uZTre1yyjkJIP6LFf3BKqm11bNCggVi+fHmdPH7F6ydE3Tl2d+7cES1bthQ7d+40qlNtPYbsApNBQUEB4uPjERoaalimVqsRGhqKuLg4BSMz7dy5c/D19UWzZs0watQoXLlyBQAQHx+PwsJCo3q0adMGTZo0MdQjLi4OHTp0gJeXl6FMWFgYsrKy8McffxjKFN+GvowS70VSUhJSUlKM4nF1dUVwcLBRndzc3NC1a1dDmdDQUKjVahw+fNhQpnfv3rCzszOUCQsLQ2JiIm7fvm0oo1S99+zZA09PT7Ru3Rqvvvoqbt68aXitttUvMzMTANCwYUMA8n0u5fwel6yj3v/+9z+4u7ujffv2iIqKQm5uruG12lJHrVaLmJgY5OTkICQkpM4dv5L106sLx27ChAkYNGhQqThq6zHkzVBlcOPGDWi1WqMDDwBeXl74888/FYrKtODgYKxcuRKtW7dGcnIy3nvvPfTq1QunT59GSkoK7Ozs4ObmZrSOl5cXUlJSAAApKSkm66l/rbwyWVlZuHv3LhwdHS1Uu9L0MZmKp3i8np6eRq/b2NigYcOGRmWaNm1aahv61xo0aFBmvfXbsJT+/fvjqaeeQtOmTXHhwgW8/fbbGDBgAOLi4qDRaGpV/XQ6HaZMmYKHHnoI7du3N+xfjs/l7du3Zfkem6ojADz33HPw9/eHr68vTp48ibfeeguJiYlYv359rajjqVOnEBISgry8PDg7O2PDhg1o27Ytjh8/XieOX1n1A2r/sQOAmJgYJCQk4Pfffy/1Wm39DjIBIiMDBgwwPO/YsSOCg4Ph7++P1atXy5qYUPV59tlnDc87dOiAjh07onnz5tizZw/69u2rYGTmmzBhAk6fPo3ffvtN6VAspqw6jh8/3vC8Q4cO8PHxQd++fXHhwgU0b95c7jDN1rp1axw/fhyZmZlYu3YtIiIisHfvXqXDqjZl1a9t27a1/thdvXoVkydPxs6dO+Hg4KB0ONWGXWAycHd3h0ajKTUiPjU1Fd7e3gpFVTlubm5o1aoVzp8/D29vbxQUFCAjI8OoTPF6eHt7m6yn/rXyyri4uMieZOljKu/YeHt7Iy0tzej1oqIi3Lp1q1rqLfdnoFmzZnB3d8f58+cNcdWG+k2cOBFbtmzB7t278cADDxiWy/W5lON7XFYdTQkODgYAo+NYk+toZ2eHFi1aICgoCNHR0ejUqRM+++yzOnP8yqqfKbXt2MXHxyMtLQ1dunSBjY0NbGxssHfvXnz++eewsbGBl5dXrTyGTIBkYGdnh6CgIMTGxhqW6XQ6xMbGGvUR10TZ2dm4cOECfHx8EBQUBFtbW6N6JCYm4sqVK4Z6hISE4NSpU0Y/qDt37oSLi4uhOTgkJMRoG/oySrwXTZs2hbe3t1E8WVlZOHz4sFGdMjIyEB8fbyjz66+/QqfTGf6QhYSEYN++fSgsLDSU2blzJ1q3bo0GDRoYytSEev/999+4efMmfHx8DHHV5PoJITBx4kRs2LABv/76a6muOLk+l5b8HldUR1OOHz8OAEbHsSbXsSSdTof8/Pw6cfzKq58pte3Y9e3bF6dOncLx48cNU9euXTFq1CjD81p5DM0eNk1VEhMTI+zt7cXKlSvFmTNnxPjx44Wbm5vRiPia4M033xR79uwRSUlJ4sCBAyI0NFS4u7uLtLQ0IYR0qmOTJk3Er7/+Ko4ePSpCQkJESEiIYX39qY79+vUTx48fFzt27BAeHh4mT3WcNm2aOHv2rFiyZIlFT4O/c+eOOHbsmDh27JgAID799FNx7NgxcfnyZSGEdBq8m5ub2LRpkzh58qR48sknTZ4G37lzZ3H48GHx22+/iZYtWxqdJp6RkSG8vLzEv/71L3H69GkRExMjnJycSp0mbmNjI+bPny/Onj0rZs2aVS2niZdXvzt37oipU6eKuLg4kZSUJHbt2iW6dOkiWrZsKfLy8mpF/V599VXh6uoq9uzZY3QacW5urqGMXJ9LS32PK6rj+fPnxfvvvy+OHj0qkpKSxKZNm0SzZs1E7969a0UdZ8yYIfbu3SuSkpLEyZMnxYwZM4RKpRK//PKLEKL2H7/y6lfbj11ZSp7ZVhuPIRMgGS1atEg0adJE2NnZie7du4tDhw4pHVIp4eHhwsfHR9jZ2YnGjRuL8PBwcf78ecPrd+/eFa+99ppo0KCBcHJyEsOGDRPJyclG27h06ZIYMGCAcHR0FO7u7uLNN98UhYWFRmV2794tAgMDhZ2dnWjWrJlYsWKFxeq0e/duAaDUFBERIYSQToV/9913hZeXl7C3txd9+/YViYmJRtu4efOmGDlypHB2dhYuLi5i7Nix4s6dO0ZlTpw4IR5++GFhb28vGjduLD766KNSsaxevVq0atVK2NnZiXbt2omtW7datH65ubmiX79+wsPDQ9ja2gp/f3/x0ksvlfpjUZPrZ6puAIw+M3J+Li3xPa6ojleuXBG9e/cWDRs2FPb29qJFixZi2rRpRteSqcl1fOGFF4S/v7+ws7MTHh4eom/fvobkR4jaf/zKq19tP3ZlKZkA1cZjqBJCCPPbjYiIiIhqL44BIiIiIqvDBIiIiIisDhMgIiIisjpMgIiIiMjqMAEiIiIiq8MEiIiIiKwOEyAiIiKyOkyAiEgWAQEBWLhwYaXL79mzByqVqtT9heq62bNnIzAwUOkwiOo8JkBEZESlUpU7zZ49u0rb/f33343uil2Rnj17Ijk5Ga6urlXanzm+/vprdOrUCc7OznBzc0Pnzp0RHR1d6fUvXboElUpluMdTeTZs2IAePXrA1dUV9evXR7t27TBlyhTD61OnTi11PyQiqn42SgdARDVLcnKy4fmqVaswc+ZMJCYmGpY5OzsbngshoNVqYWNT8Z8SDw8Ps+Kws7Ortrusl+fbb7/FlClT8Pnnn6NPnz7Iz8/HyZMncfr06WrfV2xsLMLDw/Gf//wHQ4YMgUqlwpkzZ7Bz505DGWdnZ6P3mIgspEo30CAiq7BixQrh6upqmNffd2zbtm2iS5cuwtbWVuzevVucP39eDBkyRHh6eop69eqJrl27ip07dxpty9/fXyxYsMAwD0B8/fXXYujQocLR0VG0aNFCbNq0qdS+bt++bRTLjh07RJs2bUS9evVEWFiYuH79umGdwsJCMWnSJOHq6ioaNmwopk+fLkaPHi2efPLJMuv45JNPijFjxlT4Xnz99deiTZs2wt7eXrRu3VosWbLEqC7Fpz59+pjcxuTJk8UjjzxS7n5mzZolOnXqVOa2AQh/f3/D66dOnRL9+/cX9erVE56enuL5558X6enpFdaHyNqxC4yIzDZjxgx89NFHOHv2LDp27Ijs7GwMHDgQsbGxOHbsGPr374/BgwfjypUr5W7nvffew4gRI3Dy5EkMHDgQo0aNwq1bt8osn5ubi/nz5+P777/Hvn37cOXKFUydOtXw+ty5c/G///0PK1aswIEDB5CVlYWNGzeWG4O3tzcOHTqEy5cvl1nmf//7H2bOnIn//Oc/OHv2LD788EO8++67+O677wAAR44cAQDs2rULycnJWL9+fZn7+uOPP8xqXUpOTjZM58+fR4sWLdC7d28AQEZGBh577DF07twZR48exY4dO5CamooRI0ZUevtEVkvpDIyIaq6yWoA2btxY4brt2rUTixYtMsybagF65513DPPZ2dkCgNi+fbvRvoq3AAEQ58+fN6yzZMkS4eXlZZj38vIS8+bNM8wXFRWJJk2alNsCdP36ddGjRw8BQLRq1UpERESIVatWCa1WayjTvHlz8eOPPxqt98EHH4iQkBAhhBBJSUkCgDh27Fi570l2drYYOHCgoRUnPDxcfPPNNyIvL89QpmQLkJ5OpxPDhg0TQUFBIjc31xBDv379jMpdvXpVABCJiYnlxkJk7dgCRERm69q1q9F8dnY2pk6digcffBBubm5wdnbG2bNnK2wB6tixo+F5vXr14OLigrS0tDLLOzk5oXnz5oZ5Hx8fQ/nMzEykpqaie/fuhtc1Gg2CgoLKjcHHxwdxcXE4deoUJk+ejKKiIkRERKB///7Q6XTIycnBhQsXMG7cOMP4HGdnZ8yZMwcXLlwod9sl1atXD1u3bsX58+fxzjvvwNnZGW+++Sa6d++O3Nzcctd9++23ERcXh02bNsHR0REAcOLECezevdsorjZt2gCA2bERWRsOgiYis9WrV89ofurUqdi5cyfmz5+PFi1awNHREU8//TQKCgrK3Y6tra3RvEqlgk6nM6u8EMLM6E1r37492rdvj9deew2vvPIKevXqhb1796Jt27YApDPFgoODjdbRaDRV2lfz5s3RvHlzvPjii/j3v/+NVq1aYdWqVRg7dqzJ8j/88AMWLFiAPXv2oHHjxobl2dnZGDx4MObOnVtqHR8fnyrFRmQtmAAR0X07cOAAxowZg2HDhgGQfpgvXbokawyurq7w8vLC77//bhgjo9VqkZCQYPZ1dfRJT05ODry8vODr64uLFy9i1KhRJsvb2dkZ9meugIAAODk5IScnx+TrcXFxePHFF/HVV1+hR48eRq916dIF69atQ0BAQKXOxCOie/iNIaL71rJlS6xfvx6DBw+GSqXCu+++W25LjqVMmjQJ0dHRaNGiBdq0aYNFixbh9u3bUKlUZa7z6quvwtfXF4899hgeeOABJCcnY86cOfDw8EBISAgAabD266+/DldXV/Tv3x/5+fk4evQobt++jcjISHh6esLR0RE7duzAAw88AAcHB5PXL5o9ezZyc3MxcOBA+Pv7IyMjA59//jkKCwvx+OOPlyqfkpKCYcOG4dlnn0VYWBhSUlIASC1PHh4emDBhAr7++muMHDkS06dPR8OGDXH+/HnExMRg+fLlVW6hIrIGHANERPft008/RYMGDdCzZ08MHjwYYWFh6NKli+xxvPXWWxg5ciRGjx6NkJAQODs7IywsDA4ODmWuExoaikOHDuGZZ55Bq1atMHz4cDg4OCA2NhaNGjUCALz44otYvnw5VqxYgQ4dOqBPnz5YuXIlmjZtCgCwsbHB559/jq+++gq+vr548sknTe6rT58+uHjxIkaPHo02bdpgwIABSElJwS+//ILWrVuXKv/nn38iNTUV3333HXx8fAxTt27dAAC+vr44cOAAtFot+vXrhw4dOmDKlClwc3ODWs0/70TlUYnq6kAnIqphdDodHnzwQYwYMQIffPCB0uEQUQ3CLjAiqjMuX76MX375xXBF58WLFyMpKQnPPfec0qERUQ3DNlIiqjPUajVWrlyJbt264aGHHsKpU6ewa9cuPPjgg0qHRkQ1DLvAiIiIyOqwBYiIiIisDhMgIiIisjpMgIiIiMjqMAEiIiIiq8MEiIiIiKwOEyAiIiKyOkyAiIiIyOowASIiIiKrwwSIiIiIrM7/AxAE94zPaBzaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_learning_curve(model, X_train, y_train, X_test, y_test, target, steps=50):\n",
    "    train_errors, test_errors = [], []\n",
    "    m_values = np.linspace(1, len(X_train), steps, dtype=int)\n",
    "    for m in m_values:\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_test_predict = model.predict(X_test)\n",
    "        train_errors.append(mean_absolute_error(y_train[:m], y_train_predict))\n",
    "        test_errors.append(mean_absolute_error(y_test, y_test_predict))\n",
    "    \n",
    "    plt.plot(m_values, train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(m_values, test_errors, \"b-\", linewidth=3, label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Learning Curve for {target}\")\n",
    "    plt.xlabel(\"Training Set Size\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for one of the targets as an example\n",
    "plot_learning_curve(low_models['s1_low'], X_train, y_train_low['s1_low'], X_test, y_test_low['s1_low'], 's1_low', steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\anand\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.26.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\anand\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.12.0)\n",
      "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/99.8 MB 656.4 kB/s eta 0:02:32\n",
      "   ---------------------------------------- 0.3/99.8 MB 2.2 MB/s eta 0:00:45\n",
      "   ---------------------------------------- 1.1/99.8 MB 6.3 MB/s eta 0:00:16\n",
      "    --------------------------------------- 2.0/99.8 MB 9.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 3.4/99.8 MB 12.7 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 3.8/99.8 MB 13.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 4.4/99.8 MB 12.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 4.9/99.8 MB 12.7 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 5.7/99.8 MB 12.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.5/99.8 MB 13.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.9/99.8 MB 13.0 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 6.9/99.8 MB 12.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 8.5/99.8 MB 13.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 9.2/99.8 MB 13.3 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 9.2/99.8 MB 12.8 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 10.4/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 10.9/99.8 MB 15.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 11.2/99.8 MB 14.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 11.7/99.8 MB 14.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 12.2/99.8 MB 13.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 12.5/99.8 MB 13.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 13.0/99.8 MB 12.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 13.5/99.8 MB 12.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.0/99.8 MB 11.7 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 14.5/99.8 MB 12.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 15.0/99.8 MB 11.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 15.5/99.8 MB 12.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 16.0/99.8 MB 11.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 16.5/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 17.0/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 17.5/99.8 MB 11.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 18.1/99.8 MB 11.3 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 18.6/99.8 MB 10.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 19.1/99.8 MB 10.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 19.6/99.8 MB 11.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.1/99.8 MB 10.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 20.6/99.8 MB 10.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.2/99.8 MB 10.4 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 21.7/99.8 MB 10.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.3/99.8 MB 10.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 22.8/99.8 MB 10.9 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 23.3/99.8 MB 11.1 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 23.8/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 24.4/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 25.0/99.8 MB 11.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 25.5/99.8 MB 11.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.0/99.8 MB 11.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 26.6/99.8 MB 11.5 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 27.2/99.8 MB 11.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 27.7/99.8 MB 11.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.3/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 28.9/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 29.4/99.8 MB 11.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 30.0/99.8 MB 11.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 30.6/99.8 MB 11.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 31.2/99.8 MB 11.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 31.8/99.8 MB 12.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 32.3/99.8 MB 12.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 32.9/99.8 MB 12.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 33.5/99.8 MB 12.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.2/99.8 MB 12.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 34.7/99.8 MB 12.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 35.4/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.0/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 36.6/99.8 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 37.2/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 37.8/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 38.4/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.0/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 39.6/99.8 MB 12.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 40.8/99.8 MB 12.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 41.5/99.8 MB 13.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 42.1/99.8 MB 13.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 42.8/99.8 MB 13.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 43.4/99.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 44.1/99.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 44.7/99.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 45.4/99.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 46.0/99.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 46.6/99.8 MB 13.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 47.3/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 48.0/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 48.7/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 49.3/99.8 MB 14.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.0/99.8 MB 13.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 50.6/99.8 MB 13.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 51.3/99.8 MB 13.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 51.9/99.8 MB 13.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 52.5/99.8 MB 14.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 53.1/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 53.7/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 54.3/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 54.9/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 55.5/99.8 MB 13.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 56.1/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 56.7/99.8 MB 13.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 57.2/99.8 MB 13.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 57.8/99.8 MB 13.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 58.4/99.8 MB 13.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 59.0/99.8 MB 13.1 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 59.6/99.8 MB 13.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 60.2/99.8 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 60.8/99.8 MB 12.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 61.4/99.8 MB 12.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 62.0/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 62.6/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 63.2/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 63.8/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 64.3/99.8 MB 12.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 64.9/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 65.5/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 66.1/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 66.7/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 67.3/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 67.8/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 68.4/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 69.0/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 69.6/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 71.4/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 72.0/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 72.6/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.1/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 73.7/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 74.3/99.8 MB 12.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 74.9/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 75.5/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.1/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.7/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 77.3/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.8/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.4/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.0/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.6/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.2/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.8/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.4/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.0/99.8 MB 12.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 82.5/99.8 MB 12.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.0/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 83.6/99.8 MB 12.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.2/99.8 MB 12.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.6/99.8 MB 12.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.0/99.8 MB 12.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.4/99.8 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 85.8/99.8 MB 11.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.2/99.8 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 86.7/99.8 MB 11.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 87.1/99.8 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 87.5/99.8 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.0/99.8 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.4/99.8 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 88.8/99.8 MB 10.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.3/99.8 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 89.7/99.8 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 90.1/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 90.6/99.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.0/99.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.5/99.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.9/99.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.4/99.8 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.8/99.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.3/99.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.7/99.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.2/99.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 94.7/99.8 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.2/99.8 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.7/99.8 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.1/99.8 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.6/99.8 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.1/99.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.6/99.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.1/99.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.6/99.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.2/99.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 8.7 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge - MAE: 0.041451971000136215\n",
      "Lasso - MAE: 0.0414553103\n",
      "ElasticNet - MAE: 0.0414553103\n",
      "Decision Tree - MAE: 0.05808900000000001\n",
      "Random Forest - MAE: 0.04217235\n",
      "Gradient Boosting - MAE: 0.0414616233715646\n",
      "XGBoost - MAE: 0.04215384623048454\n",
      "SVR - MAE: 0.04507545099781603\n",
      "\n",
      "Model Performance:\n",
      "Ridge: 0.041451971000136215\n",
      "Lasso: 0.0414553103\n",
      "ElasticNet: 0.0414553103\n",
      "Decision Tree: 0.05808900000000001\n",
      "Random Forest: 0.04217235\n",
      "Gradient Boosting: 0.0414616233715646\n",
      "XGBoost: 0.04215384623048454\n",
      "SVR: 0.04507545099781603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the models to compare\n",
    "models = {\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'XGBoost': XGBRegressor(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_low['s1_low'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    results[name] = mae\n",
    "    print(f'{name} - MAE: {mae}')\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nModel Performance:\")\n",
    "for name, mae in results.items():\n",
    "    print(f\"{name}: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m300\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m      9\u001b[0m }\n\u001b[0;32m     11\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mRandomForestRegressor(), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example of hyperparameter tuning with GridSearchCV for RandomForestRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '../mihiresh/commerce_logic.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Features and target columns\n",
    "features = ['years_to_retire', 'salary', 'investment_amount', 'current_savings', 'debt',\n",
    "            'other_expenses', 'number_of_dependents', 'current_invested_amount']\n",
    "\n",
    "# Targets for percentage allocations\n",
    "targets_low = ['s1_low', 's2_low', 's3_low', 's4_low', 's5_low', 's6_low']\n",
    "targets_mid = ['s1_mid', 's2_mid', 's3_mid', 's4_mid', 's5_mid', 's6_mid']\n",
    "targets_high = ['s1_high', 's2_high', 's3_high', 's4_high', 's5_high', 's6_high']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = data[features]\n",
    "y_low = data[targets_low]\n",
    "y_mid = data[targets_mid]\n",
    "y_high = data[targets_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1_low</th>\n",
       "      <th>s2_low</th>\n",
       "      <th>s3_low</th>\n",
       "      <th>s4_low</th>\n",
       "      <th>s5_low</th>\n",
       "      <th>s6_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       s1_low  s2_low  s3_low  s4_low  s5_low  s6_low\n",
       "0           9       0      22      16      42      11\n",
       "1          10       0      28      18      38       6\n",
       "2           5       3      28      17      42       5\n",
       "3          14       3      21      18      36       8\n",
       "4           4       1      26      16      44       9\n",
       "...       ...     ...     ...     ...     ...     ...\n",
       "49995      16       4      21      15      37       7\n",
       "49996      12       3      27      17      35       6\n",
       "49997       5       2      26      19      38      10\n",
       "49998      12       3      26      16      36       7\n",
       "49999       4       3      28      19      39       7\n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize target values to sum to 1\n",
    "y_low = y_low.div(y_low.sum(axis=1), axis=0)\n",
    "y_mid = y_mid.div(y_mid.sum(axis=1), axis=0)\n",
    "y_high = y_high.div(y_high.sum(axis=1), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [col for col in features]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "    ])\n",
    "\n",
    "# Transform the data\n",
    "X_processed = preprocessor.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train_low, y_test_low = train_test_split(X_processed, y_low, test_size=0.2, random_state=42)\n",
    "_, _, y_train_mid, y_test_mid = train_test_split(X_processed, y_mid, test_size=0.2, random_state=42)\n",
    "_, _, y_train_high, y_test_high = train_test_split(X_processed, y_high, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(64, activation='relu')(input_layer)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 8) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 32) dtype=float32 (created by layer 'dense_1')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layers for low, mid, and high risk levels\n",
    "low_output = Dense(6, activation='sigmoid', name='low_output')(x)\n",
    "mid_output = Dense(6, activation='sigmoid', name='mid_output')(x)\n",
    "high_output = Dense(6, activation='sigmoid', name='high_output')(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=[low_output, mid_output, high_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:From c:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0039 - low_output_loss: 0.0011 - mid_output_loss: 0.0012 - high_output_loss: 0.0016 - low_output_accuracy: 0.9984 - mid_output_accuracy: 0.8458 - high_output_accuracy: 0.5108 - val_loss: 0.0032 - val_low_output_loss: 8.9197e-04 - val_mid_output_loss: 9.6273e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 2/150\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0032 - low_output_loss: 9.0148e-04 - mid_output_loss: 9.5915e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5164 - val_loss: 0.0032 - val_low_output_loss: 8.9674e-04 - val_mid_output_loss: 9.4362e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 3/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0032 - low_output_loss: 8.9751e-04 - mid_output_loss: 9.5529e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5230 - val_loss: 0.0032 - val_low_output_loss: 8.9536e-04 - val_mid_output_loss: 9.5304e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 4/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0032 - low_output_loss: 8.9602e-04 - mid_output_loss: 9.5378e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5240 - val_loss: 0.0032 - val_low_output_loss: 8.8361e-04 - val_mid_output_loss: 9.4261e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 5/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0032 - low_output_loss: 8.9514e-04 - mid_output_loss: 9.5242e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5231 - val_loss: 0.0032 - val_low_output_loss: 8.8842e-04 - val_mid_output_loss: 9.5050e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 6/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.9408e-04 - mid_output_loss: 9.5216e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5253 - val_loss: 0.0032 - val_low_output_loss: 8.8212e-04 - val_mid_output_loss: 9.5262e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0100\n",
      "Epoch 7/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8879e-04 - mid_output_loss: 9.4568e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5399 - val_loss: 0.0031 - val_low_output_loss: 8.8082e-04 - val_mid_output_loss: 9.3849e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0020\n",
      "Epoch 8/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8878e-04 - mid_output_loss: 9.4578e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5416 - val_loss: 0.0031 - val_low_output_loss: 8.8118e-04 - val_mid_output_loss: 9.3838e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0020\n",
      "Epoch 9/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8879e-04 - mid_output_loss: 9.4561e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5415 - val_loss: 0.0031 - val_low_output_loss: 8.8088e-04 - val_mid_output_loss: 9.3808e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0020\n",
      "Epoch 10/150\n",
      "5000/5000 [==============================] - 12s 2ms/step - loss: 0.0031 - low_output_loss: 8.8900e-04 - mid_output_loss: 9.4577e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5410 - val_loss: 0.0031 - val_low_output_loss: 8.8020e-04 - val_mid_output_loss: 9.3853e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0020\n",
      "Epoch 11/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8865e-04 - mid_output_loss: 9.4573e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5394 - val_loss: 0.0031 - val_low_output_loss: 8.7973e-04 - val_mid_output_loss: 9.3856e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0020\n",
      "Epoch 12/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8808e-04 - mid_output_loss: 9.4503e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5455 - val_loss: 0.0031 - val_low_output_loss: 8.7975e-04 - val_mid_output_loss: 9.3744e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8820e-04 - mid_output_loss: 9.4501e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5463 - val_loss: 0.0031 - val_low_output_loss: 8.7989e-04 - val_mid_output_loss: 9.3702e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8808e-04 - mid_output_loss: 9.4504e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5460 - val_loss: 0.0031 - val_low_output_loss: 8.8021e-04 - val_mid_output_loss: 9.3792e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8817e-04 - mid_output_loss: 9.4488e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5450 - val_loss: 0.0031 - val_low_output_loss: 8.8004e-04 - val_mid_output_loss: 9.3874e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8820e-04 - mid_output_loss: 9.4490e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5452 - val_loss: 0.0031 - val_low_output_loss: 8.8055e-04 - val_mid_output_loss: 9.3724e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8816e-04 - mid_output_loss: 9.4485e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5462 - val_loss: 0.0031 - val_low_output_loss: 8.8040e-04 - val_mid_output_loss: 9.3764e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "5000/5000 [==============================] - 11s 2ms/step - loss: 0.0031 - low_output_loss: 8.8807e-04 - mid_output_loss: 9.4500e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5448 - val_loss: 0.0031 - val_low_output_loss: 8.8014e-04 - val_mid_output_loss: 9.3782e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8811e-04 - mid_output_loss: 9.4497e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5443 - val_loss: 0.0031 - val_low_output_loss: 8.7975e-04 - val_mid_output_loss: 9.3855e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8814e-04 - mid_output_loss: 9.4505e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5459 - val_loss: 0.0031 - val_low_output_loss: 8.8040e-04 - val_mid_output_loss: 9.3728e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8814e-04 - mid_output_loss: 9.4491e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5463 - val_loss: 0.0031 - val_low_output_loss: 8.7980e-04 - val_mid_output_loss: 9.3799e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8803e-04 - mid_output_loss: 9.4508e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5458 - val_loss: 0.0031 - val_low_output_loss: 8.8001e-04 - val_mid_output_loss: 9.3795e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8821e-04 - mid_output_loss: 9.4484e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5459 - val_loss: 0.0031 - val_low_output_loss: 8.7958e-04 - val_mid_output_loss: 9.3678e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8817e-04 - mid_output_loss: 9.4499e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5463 - val_loss: 0.0031 - val_low_output_loss: 8.8038e-04 - val_mid_output_loss: 9.3686e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8825e-04 - mid_output_loss: 9.4500e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5454 - val_loss: 0.0031 - val_low_output_loss: 8.8035e-04 - val_mid_output_loss: 9.3699e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8815e-04 - mid_output_loss: 9.4492e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5459 - val_loss: 0.0031 - val_low_output_loss: 8.8049e-04 - val_mid_output_loss: 9.3734e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8809e-04 - mid_output_loss: 9.4495e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5457 - val_loss: 0.0031 - val_low_output_loss: 8.7996e-04 - val_mid_output_loss: 9.3678e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8803e-04 - mid_output_loss: 9.4503e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5463 - val_loss: 0.0031 - val_low_output_loss: 8.8012e-04 - val_mid_output_loss: 9.3680e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "5000/5000 [==============================] - 10s 2ms/step - loss: 0.0031 - low_output_loss: 8.8812e-04 - mid_output_loss: 9.4497e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5461 - val_loss: 0.0031 - val_low_output_loss: 8.8165e-04 - val_mid_output_loss: 9.3725e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0031 - low_output_loss: 8.8821e-04 - mid_output_loss: 9.4497e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5435 - val_loss: 0.0031 - val_low_output_loss: 8.7960e-04 - val_mid_output_loss: 9.3846e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0031 - low_output_loss: 8.8819e-04 - mid_output_loss: 9.4501e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5448 - val_loss: 0.0031 - val_low_output_loss: 8.7983e-04 - val_mid_output_loss: 9.3690e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "5000/5000 [==============================] - 9s 2ms/step - loss: 0.0031 - low_output_loss: 8.8804e-04 - mid_output_loss: 9.4502e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5441 - val_loss: 0.0031 - val_low_output_loss: 8.8029e-04 - val_mid_output_loss: 9.3813e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "5000/5000 [==============================] - 8s 2ms/step - loss: 0.0031 - low_output_loss: 8.8815e-04 - mid_output_loss: 9.4490e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8496 - high_output_accuracy: 0.5459 - val_loss: 0.0031 - val_low_output_loss: 8.8030e-04 - val_mid_output_loss: 9.3871e-04 - val_high_output_loss: 0.0013 - val_low_output_accuracy: 1.0000 - val_mid_output_accuracy: 0.8489 - val_high_output_accuracy: 0.5481 - lr: 0.0010\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0031 - low_output_loss: 8.7958e-04 - mid_output_loss: 9.3678e-04 - high_output_loss: 0.0013 - low_output_accuracy: 1.0000 - mid_output_accuracy: 0.8489 - high_output_accuracy: 0.5481\n",
      "Test Low Accuracy: 1.0, Test Mid Accuracy: 0.8489000201225281, Test High Accuracy: 0.5480999946594238\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, [y_train_low, y_train_mid, y_train_high],\n",
    "                    epochs=150, batch_size=8, validation_data=(X_test, [y_test_low, y_test_mid, y_test_high]),\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "# Evaluate the model\n",
    "loss, low_loss, mid_loss, high_loss, low_acc, mid_acc, high_acc = model.evaluate(X_test, [y_test_low, y_test_mid, y_test_high])\n",
    "print(f\"Test Low Accuracy: {low_acc}, Test Mid Accuracy: {mid_acc}, Test High Accuracy: {high_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Low Risk:\n",
      "s1: 8.49%, s2: 2.55%, s3: 25.28%, s4: 17.86%, s5: 39.88%, s6: 10.34%\n",
      "Medium Risk:\n",
      "s1: 6.35%, s2: 7.86%, s3: 15.35%, s4: 25.08%, s5: 20.33%, s6: 29.84%\n",
      "High Risk:\n",
      "s1: 30.61%, s2: 30.05%, s3: 7.91%, s4: 7.94%, s5: 7.83%, s6: 20.09%\n"
     ]
    }
   ],
   "source": [
    "# Predict using the model\n",
    "low_pred, mid_pred, high_pred = model.predict(X_test)\n",
    "\n",
    "# Function to display results\n",
    "def display_results(employee_data, low_pred, mid_pred, high_pred):\n",
    "    print(\"Low Risk:\")\n",
    "    print(f\"s1: {low_pred[0]*100:.2f}%, s2: {low_pred[1]*100:.2f}%, s3: {low_pred[2]*100:.2f}%, s4: {low_pred[3]*100:.2f}%, s5: {low_pred[4]*100:.2f}%, s6: {low_pred[5]*100:.2f}%\")\n",
    "    print(\"Medium Risk:\")\n",
    "    print(f\"s1: {mid_pred[0]*100:.2f}%, s2: {mid_pred[1]*100:.2f}%, s3: {mid_pred[2]*100:.2f}%, s4: {mid_pred[3]*100:.2f}%, s5: {mid_pred[4]*100:.2f}%, s6: {mid_pred[5]*100:.2f}%\")\n",
    "    print(\"High Risk:\")\n",
    "    print(f\"s1: {high_pred[0]*100:.2f}%, s2: {high_pred[1]*100:.2f}%, s3: {high_pred[2]*100:.2f}%, s4: {high_pred[3]*100:.2f}%, s5: {high_pred[4]*100:.2f}%, s6: {high_pred[5]*100:.2f}%\")\n",
    "\n",
    "# Example new employee data\n",
    "new_employee = pd.DataFrame([[30, 45000, 12000, 35000, 2000, 3000, 1, 1]], columns=features)\n",
    "new_employee_processed = preprocessor.transform(new_employee)\n",
    "predicted_low, predicted_mid, predicted_high = model.predict(new_employee_processed)\n",
    "\n",
    "display_results(new_employee, predicted_low[0], predicted_mid[0], predicted_high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anand\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['preprocessor_pipeline.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(\"investment_recommendation.h5\")\n",
    "import joblib\n",
    "joblib.dump(preprocessor,'preprocessor_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
